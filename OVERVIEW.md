# ğŸ“˜ VisÃ£o Geral do Projeto - RAG Crawler

## ğŸ¯ Objetivo

Sistema modular e reutilizÃ¡vel para **crawling**, **scraping** e **extraÃ§Ã£o de texto** de pÃ¡ginas web e PDFs,
especificamente otimizado para preparar dados para sistemas **RAG (Retrieval-Augmented Generation)** com LLMs.

## âœ¨ CaracterÃ­sticas Principais

### 1. **Crawling Inteligente**

- Navega automaticamente por domÃ­nios e subdomÃ­nios
- Controle de profundidade e limite de pÃ¡ginas
- Respeita estrutura de links do site
- Delay configurÃ¡vel entre requisiÃ§Ãµes

### 2. **Scraping Eficiente**

- Extrai apenas texto Ãºtil (remove scripts, estilos, etc)
- Limpeza e normalizaÃ§Ã£o automÃ¡tica
- Suporte a diferentes tipos de conteÃºdo
- Headers personalizÃ¡veis

### 3. **Processamento de PDFs**

- Download e extraÃ§Ã£o automÃ¡tica
- Suporte a mÃºltiplas pÃ¡ginas
- Limite de tamanho configurÃ¡vel
- Armazenamento organizado

### 4. **PersistÃªncia Incremental**

- NÃ£o reprocessa URLs jÃ¡ extraÃ­das
- Banco de dados leve (TinyDB)
- Append incremental ao arquivo de texto
- Rastreamento de status e erros

### 5. **Preparado para RAG**

- Formato estruturado de saÃ­da
- Metadados preservados
- FÃ¡cil integraÃ§Ã£o com LangChain, LlamaIndex
- CompatÃ­vel com vector stores populares

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   main.py   â”‚  â† Script principal CLI
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WebCrawler        â”‚  â† Orquestra todo o processo
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º HTMLScraper      (extrai texto de HTML)
       â”œâ”€â”€â–º PDFExtractor     (extrai texto de PDFs)
       â”œâ”€â”€â–º URLStorage       (rastreia URLs processadas)
       â””â”€â”€â–º TextStorage      (salva texto incrementalmente)
```

### MÃ³dulos Principais

| MÃ³dulo             | Responsabilidade                             |
|--------------------|----------------------------------------------|
| `crawler.py`       | Motor de crawling, gerencia fila e navegaÃ§Ã£o |
| `scraper.py`       | ExtraÃ§Ã£o de texto de pÃ¡ginas HTML            |
| `pdf_extractor.py` | Download e extraÃ§Ã£o de texto de PDFs         |
| `storage.py`       | PersistÃªncia de URLs e texto                 |
| `utils.py`         | FunÃ§Ãµes auxiliares (normalizaÃ§Ã£o, validaÃ§Ã£o) |
| `settings.py`      | ConfiguraÃ§Ãµes centralizadas                  |

## ğŸ“Š Fluxo de Dados

```
URL Inicial
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NormalizaÃ§Ã£o â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Sim     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JÃ¡ processada?â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Pula    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ NÃ£o
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mesmo domÃ­nio?â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Sim
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Buscar HTML  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â–º HTML â”€â”€â–º Extrair Texto â”€â”€â–º Salvar
        â”‚
        â””â”€â”€â–º PDF  â”€â”€â–º Baixar/Extrair â”€â”€â–º Salvar
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extrair Links â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Adicionar Ã    â”‚
â”‚     Fila      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Estrutura de Arquivos

```
rag-crawler/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                  # CLI principal
â”œâ”€â”€ ğŸ“„ setup.py                 # Script de instalaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ example_usage.py         # Exemplos prÃ¡ticos
â”œâ”€â”€ ğŸ“„ requirements.txt         # DependÃªncias
â”œâ”€â”€ ğŸ“„ Makefile                 # Comandos Ãºteis
â”‚
â”œâ”€â”€ ğŸ“– README.md                # DocumentaÃ§Ã£o completa
â”œâ”€â”€ ğŸ“– QUICKSTART.md            # Guia rÃ¡pido
â”œâ”€â”€ ğŸ“– RAG_INTEGRATION.md       # IntegraÃ§Ã£o RAG
â”œâ”€â”€ ğŸ“– OVERVIEW.md              # Este arquivo
â”‚
â”œâ”€â”€ ğŸ“‚ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py             # ConfiguraÃ§Ãµes
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler.py              # Motor de crawling
â”‚   â”œâ”€â”€ scraper.py              # Scraping HTML
â”‚   â”œâ”€â”€ pdf_extractor.py        # ExtraÃ§Ã£o PDF
â”‚   â”œâ”€â”€ storage.py              # PersistÃªncia
â”‚   â””â”€â”€ utils.py                # Utilidades
â”‚
â”œâ”€â”€ ğŸ“‚ data/                    # Gerado em runtime
â”‚   â”œâ”€â”€ crawled_urls.json       # HistÃ³rico de URLs
â”‚   â”œâ”€â”€ text_output.txt         # Texto extraÃ­do (SAÃDA)
â”‚   â””â”€â”€ pdfs/                   # PDFs baixados
â”‚
â””â”€â”€ ğŸ“‚ logs/                    # Gerado em runtime
    â””â”€â”€ crawler.log             # Logs detalhados
```

## ğŸ”§ Tecnologias Utilizadas

### Core

- **Python 3.8+**: Linguagem base
- **Requests**: HTTP requests
- **BeautifulSoup4**: Parsing HTML
- **lxml**: Parser rÃ¡pido

### ExtraÃ§Ã£o

- **PyPDF2**: ManipulaÃ§Ã£o de PDFs
- **pdfplumber**: ExtraÃ§Ã£o avanÃ§ada de PDFs

### Armazenamento

- **TinyDB**: Banco NoSQL leve (JSON)
- **Arquivos texto**: SaÃ­da simples e portÃ¡vel

### Utilities

- **colorama**: Output colorido
- **python-dotenv**: VariÃ¡veis de ambiente

## ğŸš€ Casos de Uso

### 1. DocumentaÃ§Ã£o TÃ©cnica

```bash
# Extrair docs de bibliotecas Python
python main.py --url https://docs.python.org --max-depth 3
```

### 2. Base de Conhecimento Corporativa

```bash
# Extrair conteÃºdo interno
python main.py --url https://wiki.empresa.com --max-depth 4
```

### 3. Pesquisa e AnÃ¡lise

```bash
# Coletar artigos de blog
python main.py --url https://blog.exemplo.com --max-pages 100
```

### 4. Treinamento de Chatbots

```bash
# Extrair FAQs e suporte
python main.py --url https://suporte.empresa.com
```

## ğŸ¨ Diferenciais

### âœ… Modularidade

- Cada componente funciona independentemente
- FÃ¡cil de estender e customizar
- ReutilizÃ¡vel em outros projetos

### âœ… Simplicidade

- CÃ³digo limpo e bem documentado
- ConfiguraÃ§Ã£o via arquivo centralizado
- CLI intuitivo

### âœ… EficiÃªncia

- NÃ£o reprocessa URLs
- Append incremental
- Controle de recursos

### âœ… ProduÃ§Ã£o Ready

- Logging completo
- Tratamento de erros
- ConfiguraÃ§Ãµes flexÃ­veis
- Testes incluÃ­dos

## ğŸ“ˆ Pipeline RAG Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Crawler    â”‚
â”‚              â”‚
â”‚  1. Navega   â”‚
â”‚  2. Extrai   â”‚
â”‚  3. Salva    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ text_output  â”‚
â”‚    .txt      â”‚ â† Arquivo de saÃ­da
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunking    â”‚
â”‚              â”‚
â”‚ - Divide     â”‚
â”‚ - Overlap    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embeddings  â”‚
â”‚              â”‚
â”‚ - OpenAI     â”‚
â”‚ - Local      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Store â”‚
â”‚              â”‚
â”‚ - FAISS      â”‚
â”‚ - Pinecone   â”‚
â”‚ - Chroma     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RAG      â”‚
â”‚              â”‚
â”‚ - Retrieval  â”‚
â”‚ - Generation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ” Boas PrÃ¡ticas Implementadas

### SeguranÃ§a

- âœ… ValidaÃ§Ã£o de URLs
- âœ… Timeout em requisiÃ§Ãµes
- âœ… Limite de tamanho de arquivos
- âœ… SanitizaÃ§Ã£o de input

### Performance

- âœ… Reuso de sessÃµes HTTP
- âœ… Streaming de PDFs grandes
- âœ… Cache de URLs processadas
- âœ… Delay entre requisiÃ§Ãµes

### Manutenibilidade

- âœ… CÃ³digo modular
- âœ… ConfiguraÃ§Ã£o centralizada
- âœ… Logging estruturado
- âœ… DocumentaÃ§Ã£o completa

### Ã‰tica

- âœ… Respeita robots.txt (responsabilidade do usuÃ¡rio)
- âœ… User-Agent identificÃ¡vel
- âœ… Delay configurÃ¡vel
- âœ… Limite de pÃ¡ginas

## ğŸ› ï¸ Extensibilidade

### FÃ¡cil Adicionar

**Novo Formato de Arquivo**

```python
# Em src/, criar novo mÃ³dulo: docx_extractor.py
class DocxExtractor:
    def extract(self, url):
        # ImplementaÃ§Ã£o
        pass
```

**Novo Storage Backend**

```python
# Substituir TinyDB por PostgreSQL, MongoDB, etc
class PostgresStorage(URLStorage):
    def __init__(self, connection_string):
        # ImplementaÃ§Ã£o
        pass
```

**Filtros Customizados**

```python
# Em utils.py, adicionar validaÃ§Ãµes
def should_process_url(url, custom_rules):
    # LÃ³gica customizada
    pass
```

## ğŸ“Š MÃ©tricas e Monitoramento

O sistema rastreia automaticamente:

- âœ… URLs processadas (sucesso/falha)
- âœ… Tipos de conteÃºdo encontrados
- âœ… Tempo de processamento
- âœ… Tamanho de dados extraÃ­dos
- âœ… Erros e exceÃ§Ãµes

## ğŸ“ Aprendizados do Projeto

### Design Patterns Utilizados

- **Strategy**: Diferentes extractors (HTML, PDF)
- **Factory**: CriaÃ§Ã£o de storage backends
- **Observer**: Sistema de logging
- **Singleton**: ConfiguraÃ§Ãµes

### PrincÃ­pios SOLID

- **S**: Cada mÃ³dulo tem responsabilidade Ãºnica
- **O**: FÃ¡cil estender sem modificar cÃ³digo base
- **L**: Interfaces consistentes
- **I**: Interfaces especÃ­ficas por tipo
- **D**: Depende de abstraÃ§Ãµes, nÃ£o implementaÃ§Ãµes

## ğŸš¦ Status do Projeto

| Componente     | Status             | Notas               |
|----------------|--------------------|---------------------|
| Crawling       | âœ… Completo         | Funcional e testado |
| HTML Scraping  | âœ… Completo         | BeautifulSoup4      |
| PDF Extraction | âœ… Completo         | PyPDF2 + pdfplumber |
| Storage        | âœ… Completo         | TinyDB              |
| Logging        | âœ… Completo         | Python logging      |
| CLI            | âœ… Completo         | argparse            |
| DocumentaÃ§Ã£o   | âœ… Completo         | README, guias       |
| Testes         | âš ï¸ BÃ¡sico          | example_usage.py    |
| CI/CD          | âŒ NÃ£o implementado | -                   |

## ğŸ”® Roadmap Futuro

### Curto Prazo

- [ ] Suporte a JavaScript/SPA (Selenium)
- [ ] OCR para PDFs escaneados
- [ ] Mais tipos de arquivo (DOCX, XLSX)
- [ ] Testes unitÃ¡rios completos

### MÃ©dio Prazo

- [ ] Interface web (Flask/FastAPI)
- [ ] ParalelizaÃ§Ã£o (asyncio/multiprocessing)
- [ ] Cache inteligente
- [ ] Suporte a sitemaps

### Longo Prazo

- [ ] Scraping distribuÃ­do
- [ ] ML para detecÃ§Ã£o de conteÃºdo relevante
- [ ] Suporte a autenticaÃ§Ã£o
- [ ] Plugin system

## ğŸ“š Recursos de Aprendizado

Para entender melhor o projeto, estude:

1. **Web Scraping**: BeautifulSoup, requests
2. **Crawling**: BFS/DFS em grafos
3. **RAG**: Retrieval-Augmented Generation
4. **Vector Databases**: Embeddings, similaridade
5. **LLMs**: LangChain, LlamaIndex

## ğŸ¤ Contribuindo

Este Ã© um projeto educacional e open-source. ContribuiÃ§Ãµes sÃ£o bem-vindas!

---

**Desenvolvido com â¤ï¸ para facilitar a construÃ§Ã£o de sistemas RAG**

*VersÃ£o: 1.0.0*  
*Ãšltima atualizaÃ§Ã£o: Outubro 2025*