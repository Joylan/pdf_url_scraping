# üöÄ Guia R√°pido de In√≠cio

Comece a usar o RAG Crawler em 5 minutos!

## ‚ö° Instala√ß√£o R√°pida

```bash
# 1. Clone ou crie o diret√≥rio
mkdir rag-crawler && cd rag-crawler

# 2. Crie os arquivos do projeto
# (cole todos os arquivos fornecidos na estrutura correta)

# 3. Crie ambiente virtual
python -m venv venv

# 4. Ative o ambiente
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 5. Instale depend√™ncias
pip install -r requirements.txt
```

## üéØ Uso B√°sico

### Executar Crawler

```bash
# Crawling simples
python main.py --url https://python.org

# Com mais controle
python main.py --url https://python.org --max-depth 2 --max-pages 50

# Recome√ßar do zero
python main.py --url https://python.org --reset
```

### Ver Resultados

```bash
# Texto extra√≠do
cat data/text_output.txt

# ou no Windows
type data\text_output.txt

# URLs processadas
cat data/crawled_urls.json
```

## üìö Exemplos Pr√°ticos

### Exemplo 1: Documenta√ß√£o de Biblioteca

```bash
# Extrair documenta√ß√£o do Requests
python main.py --url https://requests.readthedocs.io --max-depth 3
```

### Exemplo 2: Blog ou Site de Not√≠cias

```bash
# Extrair artigos
python main.py --url https://realpython.com --max-depth 2 --max-pages 30
```

### Exemplo 3: Site Corporativo

```bash
# Extrair conte√∫do institucional
python main.py --url https://suaempresa.com.br --max-depth 4
```

## üß™ Testar M√≥dulos

```bash
# Executar exemplos interativos
python example_usage.py
```

Escolha op√ß√µes:

- **1**: Crawler completo (faz requisi√ß√µes reais)
- **2**: Scraping de p√°gina √∫nica
- **3**: Extra√ß√£o de PDF
- **4**: Ver URLs processadas
- **5**: Ler texto extra√≠do
- **6**: Preparar para RAG

## ü§ñ Integrar com RAG

### Op√ß√£o 1: LangChain (Recomendado)

```bash
pip install langchain langchain-openai faiss-cpu
```

```python
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import os

os.environ['OPENAI_API_KEY'] = 'sua-chave'

# Carregar texto
with open('data/text_output.txt') as f:
    text = f.read()

# Criar vector store
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts([text], embeddings)

# Buscar
docs = vectorstore.similarity_search("sua pergunta", k=3)
print(docs[0].page_content)
```

### Op√ß√£o 2: LlamaIndex

```bash
pip install llama-index
```

```python
from llama_index.core import Document, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
import os

os.environ['OPENAI_API_KEY'] = 'sua-chave'

# Carregar
with open('data/text_output.txt') as f:
    text = f.read()

# Criar √≠ndice
doc = Document(text=text)
index = VectorStoreIndex.from_documents([doc])

# Query
query_engine = index.as_query_engine()
response = query_engine.query("sua pergunta")
print(response)
```

### Op√ß√£o 3: Local (Sem API)

```bash
pip install sentence-transformers chromadb
```

```python
from sentence_transformers import SentenceTransformer
import chromadb

# Modelo local
model = SentenceTransformer('all-MiniLM-L6-v2')

# ChromaDB local
client = chromadb.Client()
collection = client.create_collection("docs")

# Carregar e indexar
with open('data/text_output.txt') as f:
    text = f.read()

collection.add(
    documents=[text],
    ids=["doc1"]
)

# Buscar
results = collection.query(
    query_texts=["sua pergunta"],
    n_results=1
)
print(results['documents'][0])
```

## ‚öôÔ∏è Configura√ß√µes Comuns

### Ajustar Velocidade

Edite `config/settings.py`:

```python
# Mais r√°pido (use com cuidado!)
DELAY_BETWEEN_REQUESTS = 0.5

# Mais devagar (mais respeitoso)
DELAY_BETWEEN_REQUESTS = 2
```

### Aumentar Limite de P√°ginas

```python
MAX_PAGES = 500  # Padr√£o: 100
MAX_DEPTH = 5  # Padr√£o: 3
```

### Ignorar Mais Tipos de Arquivo

```python
IGNORED_EXTENSIONS = {
    '.jpg', '.png', '.gif',  # Imagens
    '.mp4', '.avi',  # V√≠deos
    '.zip', '.rar',  # Arquivos
    '.xml', '.json'  # Adicione mais conforme necess√°rio
}
```

## üêõ Solu√ß√£o de Problemas

### Erro: "No module named 'lxml'"

```bash
pip install lxml
```

### Erro: "SSL Certificate Verify Failed"

```python
# Em config/settings.py, adicione:
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
```

### PDFs n√£o s√£o extra√≠dos

- Verifique se o PDF n√£o est√° protegido
- Alguns PDFs s√£o imagens escaneadas (sem texto)
- Tente reduzir `MAX_PDF_SIZE_MB`

### Muitas URLs ignoradas

- Verifique se s√£o do mesmo dom√≠nio
- Revise `IGNORED_EXTENSIONS`
- Use `--max-depth` maior

### Texto estranho no output

- Alguns sites t√™m muito JavaScript
- Tente adicionar mais tags para remover em `scraper.py`:

```python
for element in soup(['script', 'style', 'nav', 'footer',
                     'header', 'aside', 'noscript', 'iframe',
                     'button', 'form']):  # Adicione mais aqui
    element.decompose()
```

## üìä Monitoramento

### Ver Progresso em Tempo Real

```bash
# Em outro terminal
tail -f logs/crawler.log
```

### Estat√≠sticas

```python
from src.storage import URLStorage, TextStorage
from config.settings import CRAWLED_URLS_DB, TEXT_OUTPUT_FILE

url_storage = URLStorage(CRAWLED_URLS_DB)
text_storage = TextStorage(TEXT_OUTPUT_FILE)

print(f"URLs processadas: {url_storage.get_processed_count()}")
print(f"Tamanho do arquivo: {text_storage.get_file_size() / 1024:.2f} KB")
```

## üéì Pr√≥ximos Passos

1. ‚úÖ Execute o crawler no seu site alvo
2. ‚úÖ Verifique o texto extra√≠do em `data/text_output.txt`
3. ‚úÖ Escolha uma biblioteca RAG (LangChain, LlamaIndex, etc)
4. ‚úÖ Crie embeddings e vector store
5. ‚úÖ Configure seu LLM
6. ‚úÖ Construa sua aplica√ß√£o RAG!

## üìö Documenta√ß√£o Completa

- **README.md**: Documenta√ß√£o completa
- **RAG_INTEGRATION.md**: Guia detalhado de integra√ß√£o RAG
- **example_usage.py**: Exemplos de c√≥digo

## üí° Dicas R√°pidas

- **Comece pequeno**: Use `--max-pages 10` para testar
- **Respeite robots.txt**: Sempre verifique permiss√µes
- **Use delays**: Evite sobrecarregar servidores
- **Monitore logs**: Acompanhe o progresso
- **Teste incremental**: N√£o precisa resetar sempre

## üÜò Precisa de Ajuda?

1. Revise os logs em `logs/crawler.log`
2. Execute `python example_usage.py` para testar m√≥dulos
3. Verifique a se√ß√£o de Troubleshooting acima
4. Consulte o README.md completo

---

**Pronto! Voc√™ est√° preparado para come√ßar! üéâ**