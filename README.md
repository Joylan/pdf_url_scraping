# RAG Crawler - Sistema de Crawling e ExtraÃ§Ã£o de Texto

Sistema robusto de web crawling para extraÃ§Ã£o de texto de pÃ¡ginas HTML e arquivos PDF, otimizado para uso em sistemas
RAG (Retrieval-Augmented Generation).

## ğŸ†• Novidades da VersÃ£o Atualizada

### âœ… CorreÃ§Ãµes Implementadas

1. **Garantia de Processamento da URL Inicial**
    - A URL inicial passada pelo usuÃ¡rio **sempre** Ã© processada primeiro
    - Mesmo que jÃ¡ exista no banco de dados, o sistema verifica e processa se necessÃ¡rio
    - Garante que sites com subdomÃ­nios e subpÃ¡ginas sempre incluam a pÃ¡gina raiz

2. **BotÃ£o de ExportaÃ§Ã£o Completo**
    - Nova opÃ§Ã£o no menu principal (opÃ§Ã£o 3)
    - Interface interativa para exportar todo o texto coletado
    - Formato UTF-8 com cabeÃ§alho informativo
    - VerificaÃ§Ã£o de arquivos existentes antes de sobrescrever
    - EstatÃ­sticas incluÃ­das no arquivo exportado

3. **Logs Aprimorados**
    - URLs completas exibidas nos logs (limitadas a 80 caracteres para legibilidade)
    - Formato: `âœ“ [001] HTML | D0 | 12,345 chars | https://example.com/page`
    - Mais informativo e fÃ¡cil de rastrear o progresso

4. **CorreÃ§Ãµes de Avisos**
    - Type hints completos em todas as funÃ§Ãµes
    - Tratamento adequado de exceÃ§Ãµes com mensagens especÃ­ficas
    - DocumentaÃ§Ã£o (docstrings) em todos os mÃ©todos
    - ValidaÃ§Ã£o de entrada de dados

## ğŸ“‹ CaracterÃ­sticas Principais

- âœ… Crawling recursivo com controle de profundidade
- âœ… Suporte para HTML e PDF
- âœ… Logs limpos e informativos
- âœ… Armazenamento incremental
- âœ… ExportaÃ§Ã£o de dados em UTF-8
- âœ… Interface interativa via menu
- âœ… Controle de domÃ­nio e subdomÃ­nios
- âœ… Filtros de extensÃµes indesejadas
- âœ… EstatÃ­sticas detalhadas

## ğŸš€ InstalaÃ§Ã£o

### Requisitos

- Python 3.8+
- pip

### DependÃªncias

```bash
pip install requests beautifulsoup4 lxml pdfplumber tinydb
```

## ğŸ“– Como Usar

### Executar o Programa

```bash
python main.py
```

### Menu Principal

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MENU DE OPÃ‡Ã•ES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Iniciar novo crawling
2. Continuar crawling existente
3. Exportar texto coletado          â† NOVO!
4. Ver estatÃ­sticas
5. Limpar dados e reiniciar
0. Sair
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### OpÃ§Ã£o 1: Novo Crawling

1. Digite a URL inicial (ex: `https://example.com`)
2. Configure profundidade e limite de pÃ¡ginas (ou use padrÃµes)
3. O sistema processarÃ¡:
    - **Primeiro**: A URL inicial informada
    - **Depois**: Todas as subpÃ¡ginas e PDFs encontrados
4. Progresso mostrado em tempo real com URLs completas

### OpÃ§Ã£o 3: Exportar Texto ğŸ†•

1. Selecione a opÃ§Ã£o 3 no menu
2. Digite o nome do arquivo de exportaÃ§Ã£o (padrÃ£o: `export_text_output.txt`)
3. Confirme se deseja sobrescrever arquivo existente
4. O sistema cria um arquivo UTF-8 com:
    - CabeÃ§alho com estatÃ­sticas
    - Todo o texto coletado
    - URLs de origem de cada conteÃºdo
    - Timestamps de extraÃ§Ã£o

**Exemplo de SaÃ­da:**

```
================================================================================
RESULTADOS DO CRAWLING / SCRAPING
Exportado em: 2025-10-27 14:30:00
Total de pÃ¡ginas HTML: 45
Total de PDFs: 3
================================================================================

================================================================================
URL: https://example.com
Tipo: html
ExtraÃ­do em: 2025-10-27T14:25:00
================================================================================
[ConteÃºdo extraÃ­do...]

================================================================================
URL: https://example.com/documento.pdf
Tipo: pdf
ExtraÃ­do em: 2025-10-27T14:26:30
================================================================================
[ConteÃºdo do PDF...]
```

## ğŸ”§ ConfiguraÃ§Ãµes

Edite `config/settings.py` para personalizar:

```python
MAX_DEPTH = 5  # Profundidade mÃ¡xima de crawling
MAX_PAGES = 100  # NÃºmero mÃ¡ximo de pÃ¡ginas
DELAY_BETWEEN_REQUESTS = 0.8  # Delay entre requisiÃ§Ãµes (segundos)
TIMEOUT = 10  # Timeout de requisiÃ§Ãµes
MAX_PDF_SIZE_MB = 50  # Tamanho mÃ¡ximo de PDF

# ExtensÃµes ignoradas
IGNORED_EXTENSIONS = {
    '.jpg', '.jpeg', '.png', '.gif', '.svg',
    '.css', '.js', '.mp4', '.zip', ...
}
```

## ğŸ“Š Exemplo de Log

```
ğŸš€ Iniciando crawl: https://example.com
ğŸ“Š Limite: 100 pÃ¡ginas restantes | Profundidade: 5

ğŸ“ Processando URL inicial: https://example.com
âœ“ [  1] HTML | D0 | 15,234 chars | https://example.com
âœ“ [  2] HTML | D1 |  8,456 chars | https://example.com/about
âœ“ [  3] HTML | D1 | 12,789 chars | https://example.com/services
âœ“ [  1] PDF  | D2 | 45,123 chars | https://example.com/docs/manual.pdf

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“„ PÃ¡ginas HTML: 3 | ğŸ“‘ PDFs: 1 | ğŸ”— Total URLs: 4
ğŸ’¾ Tamanho: 156.78 KB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

## ğŸ“ Estrutura de DiretÃ³rios

```
rag-crawler/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py           # ConfiguraÃ§Ãµes do projeto
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler.py            # âœ¨ ATUALIZADO - LÃ³gica principal
â”‚   â”œâ”€â”€ scraper.py            # âœ¨ ATUALIZADO - ExtraÃ§Ã£o HTML
â”‚   â”œâ”€â”€ pdf_extractor.py      # âœ¨ ATUALIZADO - ExtraÃ§Ã£o PDF
â”‚   â”œâ”€â”€ storage.py            # âœ¨ ATUALIZADO - Armazenamento
â”‚   â””â”€â”€ utils.py              # âœ¨ ATUALIZADO - FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ crawled_urls.json     # Banco de URLs processadas
â”‚   â”œâ”€â”€ text_output.txt       # Texto extraÃ­do
â”‚   â””â”€â”€ pdfs/                 # PDFs baixados
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ crawler.log           # Arquivo de log detalhado
â”‚
â”œâ”€â”€ main.py                   # âœ¨ ATUALIZADO - Script principal com menu
â””â”€â”€ README.md                 # âœ¨ ATUALIZADO - Este arquivo
```

## ğŸ¯ Fluxo de Trabalho

1. **InicializaÃ§Ã£o**: URL inicial Ã© normalizada e validada
2. **Processamento da URL Inicial**: Garante que a pÃ¡gina raiz seja sempre incluÃ­da
3. **Descoberta de Links**: Extrai todos os links da pÃ¡gina
4. **Filtragem**: Remove links externos e extensÃµes indesejadas
5. **Processamento Recursivo**: Processa subpÃ¡ginas respeitando profundidade
6. **ExtraÃ§Ã£o de Texto**: Limpa e formata o conteÃºdo
7. **Armazenamento**: Salva incrementalmente em arquivo UTF-8
8. **ExportaÃ§Ã£o**: Gera arquivo final com todos os dados coletados

## ğŸ›¡ï¸ Tratamento de Erros

- âœ… Timeouts de requisiÃ§Ã£o configurÃ¡veis
- âœ… ValidaÃ§Ã£o de tipo de conteÃºdo
- âœ… VerificaÃ§Ã£o de tamanho de PDF
- âœ… Logs de debug para diagnÃ³stico
- âœ… RecuperaÃ§Ã£o de falhas individuais
- âœ… Modo de continuaÃ§Ã£o preserva progresso

## ğŸ“ Notas Importantes

### Garantia de URL Inicial

O sistema **sempre** processa a URL inicial antes de qualquer outra pÃ¡gina, garantindo que:

- Sites com subdomÃ­nios incluam a pÃ¡gina raiz
- O conteÃºdo principal seja sempre capturado
- SubpÃ¡ginas sejam processadas na ordem correta

### ExportaÃ§Ã£o de Dados

- Arquivo gerado em **UTF-8** para compatibilidade universal
- Inclui metadados de cada extraÃ§Ã£o
- Formato otimizado para sistemas RAG
- Preserva estrutura e URLs de origem

### Performance

- Delay entre requisiÃ§Ãµes evita sobrecarga
- Processamento incremental economiza memÃ³ria
- Cache de URLs evita reprocessamento
- Logs otimizados para nÃ£o poluir console

## ğŸ¤ ContribuiÃ§Ãµes

Melhorias e sugestÃµes sÃ£o bem-vindas!

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob licenÃ§a MIT.

---

**VersÃ£o**: 1.1.0  
**Ãšltima AtualizaÃ§Ã£o**: Outubro 2025  
**Autor**: Joylan Nunes Maciel