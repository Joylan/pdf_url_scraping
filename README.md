# RAG Crawler - Web Scraping para RAG com LLM

Sistema modular de crawling, scraping e extraÃ§Ã£o de texto de pÃ¡ginas web e PDFs, otimizado para preparar dados para
sistemas RAG (Retrieval-Augmented Generation) com LLMs.

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida

```bash
# 1. Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate no Windows

# 2. Instalar dependÃªncias
pip install -r requirements.txt
```

## ğŸ“– Uso BÃ¡sico

```bash
# Executar crawler
python main.py --url https://example.com

# Com opÃ§Ãµes
python main.py --url https://example.com --max-depth 3 --max-pages 50

# Resetar e recomeÃ§ar
python main.py --url https://example.com --reset
```

## ğŸ“ Estrutura

```
rag-crawler/
â”œâ”€â”€ config/settings.py      # ConfiguraÃ§Ãµes
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawler.py         # Motor de crawling
â”‚   â”œâ”€â”€ scraper.py         # ExtraÃ§Ã£o HTML
â”‚   â”œâ”€â”€ pdf_extractor.py   # ExtraÃ§Ã£o PDF
â”‚   â”œâ”€â”€ storage.py         # PersistÃªncia
â”‚   â””â”€â”€ utils.py           # Utilidades
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ text_output.txt    # Texto extraÃ­do (SAÃDA)
â”‚   â””â”€â”€ crawled_urls.json  # URLs processadas
â””â”€â”€ main.py                # Script principal
```

## âœ¨ CaracterÃ­sticas

- âœ… Crawling inteligente com controle de profundidade
- âœ… Scraping incremental (nÃ£o reprocessa URLs)
- âœ… Suporte a PDFs
- âœ… Texto limpo e estruturado para RAG
- âœ… Logging completo
- âœ… Modular e reutilizÃ¡vel

## ğŸ“š DocumentaÃ§Ã£o

O texto extraÃ­do fica em `data/text_output.txt` no formato:

```
================================================================================
URL: https://example.com/pagina1
Tipo: html
ExtraÃ­do em: 2025-10-18T10:30:00
================================================================================
[Texto extraÃ­do...]
```

Pronto para uso com LangChain, LlamaIndex ou qualquer sistema RAG!

## âš™ï¸ ConfiguraÃ§Ã£o

Edite `config/settings.py` para ajustar:

- MAX_DEPTH: Profundidade mÃ¡xima de crawling
- MAX_PAGES: NÃºmero mÃ¡ximo de pÃ¡ginas
- DELAY_BETWEEN_REQUESTS: Delay entre requisiÃ§Ãµes
- IGNORED_EXTENSIONS: ExtensÃµes para ignorar

## ğŸ› SoluÃ§Ã£o de Problemas

- **Erro de importaÃ§Ã£o**: Verifique se todas as dependÃªncias estÃ£o instaladas
- **PDFs vazios**: Alguns PDFs podem ser imagens escaneadas
- **Muitas URLs ignoradas**: Verifique IGNORED_EXTENSIONS e max-depth

## ğŸ“„ LicenÃ§a

Projeto open-source usando bibliotecas MIT/Apache 2.0.